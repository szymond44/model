{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56dfa07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5419129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.api_fetcher import ApiFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49b19542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['home_fga', 'away_fga', 'home_fg_pct', 'away_fg_pct', 'home_fg3a',\n",
      "       'away_fg3a', 'home_fg3_pct', 'away_fg3_pct', 'home_oreb', 'away_oreb',\n",
      "       'home_dreb', 'away_dreb', 'home_ast', 'away_ast', 'home_stl',\n",
      "       'away_stl', 'home_blk', 'away_blk', 'home_tov', 'away_tov', 'home_pf',\n",
      "       'away_pf', 'home_pts', 'away_pts', 'home_team', 'away_team',\n",
      "       'home_team_id', 'away_team_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "api = ApiFetcher(starting_year=2014, ending_year=2025)\n",
    "df = api.get_dataframe(numeric=False, date=False, time_coeff=False, ids=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eef4ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be43e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prep_df(df, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare basketball game data for neural network training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with basketball game statistics\n",
    "    test_size : float, default=0.2\n",
    "        Proportion of data to use for testing\n",
    "    val_size : float, default=0.2\n",
    "        Proportion of remaining data to use for validation (after test split)\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducible splits\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing:\n",
    "        - 'X_train': Training features (scaled)\n",
    "        - 'X_val': Validation features (scaled)\n",
    "        - 'X_test': Test features (scaled)\n",
    "        - 'y_train': Training labels\n",
    "        - 'y_val': Validation labels\n",
    "        - 'y_test': Test labels\n",
    "        - 'feature_names': List of feature names\n",
    "        - 'scaler': Fitted StandardScaler object\n",
    "        - 'label_encoder': Fitted LabelEncoder object (if applicable)\n",
    "        - 'n_features': Number of input features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Feature Engineering\n",
    "    # Calculate differential stats (home - away)\n",
    "    feature_pairs = [\n",
    "        ('fga', 'Field Goal Attempts'),\n",
    "        ('fg_pct', 'Field Goal Percentage'),\n",
    "        ('fg3a', '3-Point Attempts'),\n",
    "        ('fg3_pct', '3-Point Percentage'),\n",
    "        ('oreb', 'Offensive Rebounds'),\n",
    "        ('dreb', 'Defensive Rebounds'),\n",
    "        ('ast', 'Assists'),\n",
    "        ('stl', 'Steals'),\n",
    "        ('blk', 'Blocks'),\n",
    "        ('tov', 'Turnovers'),\n",
    "        ('pf', 'Personal Fouls'),\n",
    "        ('pts', 'Points')\n",
    "    ]\n",
    "    \n",
    "    # Create differential features\n",
    "    for stat, desc in feature_pairs:\n",
    "        home_col = f'home_{stat}'\n",
    "        away_col = f'away_{stat}'\n",
    "        diff_col = f'{stat}_diff'\n",
    "        data[diff_col] = data[home_col] - data[away_col]\n",
    "    \n",
    "    # Create efficiency metrics\n",
    "    data['home_total_reb'] = data['home_oreb'] + data['home_dreb']\n",
    "    data['away_total_reb'] = data['away_oreb'] + data['away_dreb']\n",
    "    data['reb_diff'] = data['home_total_reb'] - data['away_total_reb']\n",
    "    \n",
    "    # Create possession estimates (simplified)\n",
    "    data['home_poss_est'] = data['home_fga'] + 0.44 * data['home_fga'] - data['home_oreb'] + data['home_tov']\n",
    "    data['away_poss_est'] = data['away_fga'] + 0.44 * data['away_fga'] - data['away_oreb'] + data['away_tov']\n",
    "    data['poss_diff'] = data['home_poss_est'] - data['away_poss_est']\n",
    "    \n",
    "    # Create efficiency ratings\n",
    "    data['home_off_eff'] = (data['home_pts'] / data['home_poss_est']) * 100\n",
    "    data['away_off_eff'] = (data['away_pts'] / data['away_poss_est']) * 100\n",
    "    data['off_eff_diff'] = data['home_off_eff'] - data['away_off_eff']\n",
    "    \n",
    "    # Create target variable (1 if home team wins, 0 if away team wins)\n",
    "    data['home_win'] = (data['home_pts'] > data['away_pts']).astype(int)\n",
    "    \n",
    "    # Define features to use\n",
    "    base_features = [\n",
    "        'home_fga', 'away_fga', 'home_fg_pct', 'away_fg_pct',\n",
    "        'home_fg3a', 'away_fg3a', 'home_fg3_pct', 'away_fg3_pct',\n",
    "        'home_oreb', 'away_oreb', 'home_dreb', 'away_dreb',\n",
    "        'home_ast', 'away_ast', 'home_stl', 'away_stl',\n",
    "        'home_blk', 'away_blk', 'home_tov', 'away_tov',\n",
    "        'home_pf', 'away_pf'\n",
    "    ]\n",
    "    \n",
    "    differential_features = [f'{stat}_diff' for stat, _ in feature_pairs]\n",
    "    \n",
    "    engineered_features = [\n",
    "        'reb_diff', 'poss_diff', 'off_eff_diff',\n",
    "        'home_total_reb', 'away_total_reb'\n",
    "    ]\n",
    "    \n",
    "    # Combine all features\n",
    "    feature_columns = base_features + differential_features + engineered_features\n",
    "    \n",
    "    # Handle any infinite or NaN values\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # For any remaining NaN values, fill with median\n",
    "    for col in feature_columns:\n",
    "        if col in data.columns:\n",
    "            median_val = data[col].median()\n",
    "            data[col] = data[col].fillna(median_val)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = data[feature_columns].values\n",
    "    y = data['home_win'].values\n",
    "    \n",
    "    # First split: separate test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation from remaining data\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # Adjust val_size for remaining data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Prepare return dictionary\n",
    "    result = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_val': X_val_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': feature_columns,\n",
    "        'scaler': scaler,\n",
    "        'n_features': len(feature_columns),\n",
    "        'train_size': len(X_train_scaled),\n",
    "        'val_size': len(X_val_scaled),\n",
    "        'test_size': len(X_test_scaled)\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Data preparation complete!\")\n",
    "    print(f\"Total samples: {len(data)}\")\n",
    "    print(f\"Features: {len(feature_columns)}\")\n",
    "    print(f\"Training samples: {len(X_train_scaled)}\")\n",
    "    print(f\"Validation samples: {len(X_val_scaled)}\")\n",
    "    print(f\"Test samples: {len(X_test_scaled)}\")\n",
    "    print(f\"Home team win rate: {y.mean():.3f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "#prepared_data = prep_df(df1)\n",
    "#X_train, y_train = prepared_data['X_train'], prepared_data['y_train']\n",
    "#X_val, y_val = prepared_data['X_val'], prepared_data['y_val']\n",
    "#X_test, y_test = prepared_data['X_test'], prepared_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee7e1c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete!\n",
      "Total samples: 13203\n",
      "Features: 39\n",
      "Training samples: 7921\n",
      "Validation samples: 2641\n",
      "Test samples: 2641\n",
      "Home team win rate: 0.566\n",
      "Using device: cuda\n",
      "Model created with 39 input features\n",
      "Embedding dimension: 8\n",
      "Model parameters: 5065\n",
      "Epoch   0 | Train Loss: 0.4086 | Val Loss: 0.1343 | Val Acc: 0.9512\n",
      "Epoch  10 | Train Loss: 0.0158 | Val Loss: 0.0158 | Val Acc: 0.9939\n",
      "Epoch  20 | Train Loss: 0.0052 | Val Loss: 0.0076 | Val Acc: 0.9973\n",
      "Epoch  30 | Train Loss: 0.0035 | Val Loss: 0.0136 | Val Acc: 0.9955\n",
      "Epoch  40 | Train Loss: 0.0022 | Val Loss: 0.0004 | Val Acc: 1.0000\n",
      "Epoch  50 | Train Loss: 0.0057 | Val Loss: 0.0145 | Val Acc: 0.9962\n",
      "Epoch  60 | Train Loss: 0.0003 | Val Loss: 0.0004 | Val Acc: 1.0000\n",
      "Epoch  70 | Train Loss: 0.0009 | Val Loss: 0.0001 | Val Acc: 1.0000\n",
      "Epoch  80 | Train Loss: 0.0009 | Val Loss: 0.0011 | Val Acc: 0.9996\n",
      "Epoch  90 | Train Loss: 0.0005 | Val Loss: 0.0000 | Val Acc: 1.0000\n",
      "\n",
      "Test Results:\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Away Win       1.00      1.00      1.00      1145\n",
      "    Home Win       1.00      1.00      1.00      1496\n",
      "\n",
      "    accuracy                           1.00      2641\n",
      "   macro avg       1.00      1.00      1.00      2641\n",
      "weighted avg       1.00      1.00      1.00      2641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BasketballEmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network that learns 8-dimensional embeddings for basketball game data\n",
    "    and uses them for binary classification (home team win/loss prediction).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim=8, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.3):\n",
    "        super(BasketballEmbeddingNet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim2, embedding_dim)  # 8-dimensional embedding\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get embeddings\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(embeddings)\n",
    "        \n",
    "        return output, embeddings\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Extract embeddings without classification\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.embedding_layer(x)\n",
    "        return embeddings\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the basketball embedding model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(batch_x)\n",
    "            loss = criterion(outputs.squeeze(), batch_y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs, _ = model(batch_x)\n",
    "                loss = criterion(outputs.squeeze(), batch_y.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = (outputs.squeeze() > 0.5).float()\n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_targets.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_basketball_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}')\n",
    "        \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_basketball_model.pth'))\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_targets = []\n",
    "    test_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            outputs, embeddings = model(batch_x)\n",
    "            predictions = (outputs.squeeze() > 0.5).float()\n",
    "            \n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "            test_targets.extend(batch_y.cpu().numpy())\n",
    "            test_embeddings.extend(embeddings.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_targets, test_predictions, \n",
    "                              target_names=['Away Win', 'Home Win']))\n",
    "    \n",
    "    return {\n",
    "        'predictions': test_predictions,\n",
    "        'targets': test_targets,\n",
    "        'embeddings': np.array(test_embeddings),\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "def create_model_and_train(prepared_data, batch_size=64, epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Complete pipeline to create and train the model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get data\n",
    "    X_train = torch.FloatTensor(prepared_data['X_train'])\n",
    "    y_train = torch.LongTensor(prepared_data['y_train'])\n",
    "    X_val = torch.FloatTensor(prepared_data['X_val'])\n",
    "    y_val = torch.LongTensor(prepared_data['y_val'])\n",
    "    X_test = torch.FloatTensor(prepared_data['X_test'])\n",
    "    y_test = torch.LongTensor(prepared_data['y_test'])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    input_dim = prepared_data['n_features']\n",
    "    model = BasketballEmbeddingNet(input_dim=input_dim)\n",
    "    \n",
    "    print(f\"Model created with {input_dim} input features\")\n",
    "    print(f\"Embedding dimension: 8\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(model, train_loader, val_loader, epochs=epochs, \n",
    "                         learning_rate=learning_rate, device=device)\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_results = evaluate_model(model, test_loader, device=device)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_results': test_results,\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "prepared_data = prep_df(df1)  # From previous function\n",
    "results = create_model_and_train(prepared_data)\n",
    "model = results['model']\n",
    " \n",
    "# Extract embeddings for new data\n",
    "# new_embeddings = model.get_embeddings(torch.FloatTensor(new_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
