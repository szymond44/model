{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d3fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d182eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.api_fetcher import ApiFetcher\n",
    "from model.team_embeddings import EmbeddingFetcher\n",
    "from model.team_embeddings import TeamEmbeddingsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba53cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ApiFetcher(starting_year=2019, ending_year=2025)\n",
    "df = api.get_dataframe(numeric=False, date=True, time_coeff=False, ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7955d2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\szymo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 18.10117064622039\n"
     ]
    }
   ],
   "source": [
    "trainer = TeamEmbeddingsModel(df)\n",
    "test_mse, trained_model = trainer.train()\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771d0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_embeddings = trained_model.home_embedding.weight.detach().cpu().numpy()\n",
    "away_embeddings = trained_model.away_embedding.weight.detach().cpu().numpy()\n",
    "embeddings_fetcher = EmbeddingFetcher(home_embeddings, away_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a3a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df1, target_cols=['home_pts', 'away_pts'], scaler=None):\n",
    "    team_id_cols = ['home_team_id', 'away_team_id']\n",
    "    exclude_cols = target_cols + team_id_cols + ['date', 'home_team', 'away_team']\n",
    "    numeric_cols = [col for col in df1.columns if col not in exclude_cols]\n",
    "\n",
    "\n",
    "    X_numeric_raw = df1[numeric_cols].values\n",
    "    X_team_ids = df1[team_id_cols].astype(int).values\n",
    "    y = df1[target_cols].sum(axis=1).values\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_numeric = scaler.fit_transform(X_numeric_raw)\n",
    "    else:\n",
    "        X_numeric = scaler.transform(X_numeric_raw)\n",
    "\n",
    "    return X_numeric, X_team_ids, y, scaler, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6c4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAEmbeddingDataset(Dataset):\n",
    "    def __init__(self, X_numeric, X_team_ids, y, fetcher):\n",
    "        self.X_numeric = torch.tensor(X_numeric, dtype=torch.float32)\n",
    "        self.X_team_ids = torch.tensor(X_team_ids, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.fetcher = fetcher\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        numeric_features = self.X_numeric[idx]\n",
    "        home_id, away_id = self.X_team_ids[idx]\n",
    "        # Fetch embeddings\n",
    "        home_emb = torch.tensor(self.fetcher.get_home_embedding(home_id), dtype=torch.float32)\n",
    "        away_emb = torch.tensor(self.fetcher.get_away_embedding(away_id), dtype=torch.float32)\n",
    "        return numeric_features, home_emb, away_emb, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a93920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniNN(nn.Module):\n",
    "    def __init__(self, num_numeric_features, embedding_dim):\n",
    "        super(MiniNN, self).__init__()\n",
    "        input_size = num_numeric_features + embedding_dim*2\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, numeric_features, home_emb, away_emb):\n",
    "        x = torch.cat([numeric_features, home_emb, away_emb], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ed52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, fetcher, embedding_dim, num_epochs, lr, batch_size=64,):\n",
    "    # Sort and split\n",
    "    df_sorted = df.sort_values(\"date\")\n",
    "    train_df, val_df, test_df = np.split(\n",
    "        df_sorted, \n",
    "        [int(0.7*len(df_sorted)), int(0.85*len(df_sorted))]\n",
    "    )\n",
    "    \n",
    "    X_train_num, X_train_ids, y_train, scaler, numeric_cols = prep_df(train_df)\n",
    "    X_val_num, X_val_ids, y_val, _, _ = prep_df(val_df, scaler=scaler)\n",
    "    X_test_num, X_test_ids, y_test, _, _ = prep_df(test_df, scaler=scaler)\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = NBAEmbeddingDataset(X_train_num, X_train_ids, y_train, fetcher)\n",
    "    val_dataset = NBAEmbeddingDataset(X_val_num, X_val_ids, y_val, fetcher)\n",
    "    test_dataset = NBAEmbeddingDataset(X_test_num, X_test_ids, y_test, fetcher)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    num_numeric_features = X_train_num.shape[1]\n",
    "    model = MiniNN(num_numeric_features, embedding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_num, home_emb, away_emb, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_num, home_emb, away_emb)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_num.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_num, home_emb, away_emb, y in val_loader:\n",
    "                pred = model(X_num, home_emb, away_emb)\n",
    "                val_loss += criterion(pred, y).item() * X_num.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train MSE: {train_loss:.4f} | Val MSE: {val_loss:.4f}\")\n",
    "    \n",
    "    # Test MSE\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_num, home_emb, away_emb, y in test_loader:\n",
    "            pred = model(X_num, home_emb, away_emb)\n",
    "            test_loss += criterion(pred, y).item() * X_num.size(0)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test MSE: {test_loss:.4f}\")\n",
    "    \n",
    "    return model, scaler, numeric_cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51cdf854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\szymo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/270 | Train MSE: 94.7155 | Val MSE: 104.5811\n",
      "Epoch 20/270 | Train MSE: 52.1111 | Val MSE: 61.6737\n",
      "Epoch 30/270 | Train MSE: 40.3554 | Val MSE: 49.8350\n",
      "Epoch 40/270 | Train MSE: 35.5401 | Val MSE: 46.2402\n",
      "Epoch 50/270 | Train MSE: 33.5514 | Val MSE: 46.2981\n",
      "Epoch 60/270 | Train MSE: 31.9704 | Val MSE: 45.3142\n",
      "Epoch 70/270 | Train MSE: 29.5589 | Val MSE: 42.8878\n",
      "Epoch 80/270 | Train MSE: 27.8731 | Val MSE: 48.1384\n",
      "Epoch 90/270 | Train MSE: 27.5819 | Val MSE: 41.9689\n",
      "Epoch 100/270 | Train MSE: 25.8668 | Val MSE: 43.2711\n",
      "Epoch 110/270 | Train MSE: 26.0287 | Val MSE: 42.3025\n",
      "Epoch 120/270 | Train MSE: 24.7638 | Val MSE: 40.8628\n",
      "Epoch 130/270 | Train MSE: 24.7923 | Val MSE: 54.5419\n",
      "Epoch 140/270 | Train MSE: 23.1098 | Val MSE: 44.1836\n",
      "Epoch 150/270 | Train MSE: 22.8135 | Val MSE: 43.0347\n",
      "Epoch 160/270 | Train MSE: 23.1952 | Val MSE: 42.0540\n",
      "Epoch 170/270 | Train MSE: 22.7479 | Val MSE: 43.4998\n",
      "Epoch 180/270 | Train MSE: 22.1200 | Val MSE: 43.3121\n",
      "Epoch 190/270 | Train MSE: 20.6644 | Val MSE: 41.6035\n",
      "Epoch 200/270 | Train MSE: 20.5745 | Val MSE: 43.7239\n",
      "Epoch 210/270 | Train MSE: 19.7027 | Val MSE: 42.3141\n",
      "Epoch 220/270 | Train MSE: 20.1902 | Val MSE: 43.2478\n",
      "Epoch 230/270 | Train MSE: 21.4927 | Val MSE: 45.2132\n",
      "Epoch 240/270 | Train MSE: 19.3465 | Val MSE: 43.5631\n",
      "Epoch 250/270 | Train MSE: 19.2311 | Val MSE: 43.7716\n",
      "Epoch 260/270 | Train MSE: 19.7024 | Val MSE: 44.6969\n",
      "Epoch 270/270 | Train MSE: 18.9682 | Val MSE: 44.9783\n",
      "Test MSE: 50.9683\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = home_embeddings.shape[1]\n",
    "model, scaler, numeric_cols = train_model(df, embeddings_fetcher, embedding_dim, num_epochs=270, lr=0.0008991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf99845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch 10/250 | Train MSE: 201.0823 | Val MSE: 223.0521\n",
      "Epoch 20/250 | Train MSE: 170.4271 | Val MSE: 204.7904\n",
      "Epoch 30/250 | Train MSE: 144.9442 | Val MSE: 194.1804\n",
      "Epoch 40/250 | Train MSE: 127.8007 | Val MSE: 193.4398\n",
      "Epoch 50/250 | Train MSE: 114.6544 | Val MSE: 194.5228\n",
      "Epoch 60/250 | Train MSE: 102.3096 | Val MSE: 199.7999\n",
      "Epoch 70/250 | Train MSE: 94.2005 | Val MSE: 207.7082\n",
      "Epoch 80/250 | Train MSE: 86.6635 | Val MSE: 210.0368\n",
      "Epoch 90/250 | Train MSE: 81.5413 | Val MSE: 217.1168\n",
      "Epoch 100/250 | Train MSE: 77.3566 | Val MSE: 223.1875\n",
      "Epoch 110/250 | Train MSE: 73.4958 | Val MSE: 225.7424\n",
      "Epoch 120/250 | Train MSE: 69.1060 | Val MSE: 230.6674\n",
      "Epoch 130/250 | Train MSE: 65.8082 | Val MSE: 239.0515\n",
      "Epoch 140/250 | Train MSE: 63.3244 | Val MSE: 240.6530\n",
      "Epoch 150/250 | Train MSE: 60.0522 | Val MSE: 243.6011\n",
      "Epoch 160/250 | Train MSE: 57.6055 | Val MSE: 250.0422\n",
      "Epoch 170/250 | Train MSE: 55.0881 | Val MSE: 256.0124\n",
      "Epoch 180/250 | Train MSE: 52.0146 | Val MSE: 256.2340\n",
      "Epoch 190/250 | Train MSE: 50.5064 | Val MSE: 259.5046\n",
      "Epoch 200/250 | Train MSE: 47.1253 | Val MSE: 263.3941\n",
      "Epoch 210/250 | Train MSE: 45.2521 | Val MSE: 268.0552\n",
      "Epoch 220/250 | Train MSE: 42.6344 | Val MSE: 273.8157\n",
      "Epoch 230/250 | Train MSE: 40.4542 | Val MSE: 273.3419\n",
      "Epoch 240/250 | Train MSE: 38.3579 | Val MSE: 280.1606\n",
      "Epoch 250/250 | Train MSE: 36.1864 | Val MSE: 281.4081\n",
      "Test Results:\n",
      "  Overall MSE (avg of home/away): 286.5736\n",
      "  Home Points MSE: 281.0119\n",
      "  Away Points MSE: 292.1353\n",
      "  Total Points MSE (sum): 787.5422\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.api_fetcher import ApiFetcher\n",
    "from model.team_embeddings import EmbeddingFetcher\n",
    "from model.team_embeddings import TeamEmbeddingsModel\n",
    "\n",
    "api = ApiFetcher(starting_year=2019, ending_year=2025)\n",
    "df = api.get_dataframe(numeric=False, date=True, time_coeff=False, ids=True)\n",
    "\n",
    "home_embeddings = trained_model.home_embedding.weight.detach().cpu().numpy()\n",
    "away_embeddings = trained_model.away_embedding.weight.detach().cpu().numpy()\n",
    "embeddings_fetcher = EmbeddingFetcher(home_embeddings, away_embeddings)\n",
    "\n",
    "def prep_df(df1, target_cols=['home_pts', 'away_pts'], scaler=None):\n",
    "    team_id_cols = ['home_team_id', 'away_team_id']\n",
    "    exclude_cols = target_cols + team_id_cols + ['date', 'home_team', 'away_team']\n",
    "    numeric_cols = [col for col in df1.columns if col not in exclude_cols]\n",
    "    \n",
    "    X_numeric_raw = df1[numeric_cols].values\n",
    "    X_team_ids = df1[team_id_cols].astype(int).values\n",
    "    y = df1[target_cols].values  # Shape: (n_samples, 2)\n",
    "    \n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_numeric = scaler.fit_transform(X_numeric_raw)\n",
    "    else:\n",
    "        X_numeric = scaler.transform(X_numeric_raw)\n",
    "    \n",
    "    return X_numeric, X_team_ids, y, scaler, numeric_cols\n",
    "\n",
    "# ------------------ SEKWENCYJNY DATASET ------------------\n",
    "class NBASeqDataset(Dataset):\n",
    "    def __init__(self, X_numeric, X_team_ids, y, fetcher, seq_len=3):\n",
    "        self.X_numeric = torch.tensor(X_numeric, dtype=torch.float32)\n",
    "        self.X_team_ids = torch.tensor(X_team_ids, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.fetcher = fetcher\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Pobranie sekwencji\n",
    "        seq_numeric = self.X_numeric[idx:idx+self.seq_len].flatten()\n",
    "        seq_home_emb = torch.cat([\n",
    "            torch.tensor(self.fetcher.get_home_embedding(int(self.X_team_ids[i,0])), dtype=torch.float32)\n",
    "            for i in range(idx, idx+self.seq_len)\n",
    "        ])\n",
    "        seq_away_emb = torch.cat([\n",
    "            torch.tensor(self.fetcher.get_away_embedding(int(self.X_team_ids[i,1])), dtype=torch.float32)\n",
    "            for i in range(idx, idx+self.seq_len)\n",
    "        ])\n",
    "        X_seq = torch.cat([seq_numeric, seq_home_emb, seq_away_emb])\n",
    "        y_target = self.y[idx+self.seq_len]  # przewidujemy wektor w chwili t\n",
    "        return X_seq, y_target\n",
    "\n",
    "# ------------------ SEKWENCYJNY NN ------------------\n",
    "class MiniNNSeq(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MiniNNSeq, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ------------------ FUNKCJA TRENINGU ------------------\n",
    "def train_model_seq(df, fetcher, embedding_dim, num_epochs=250, lr=0.001, batch_size=64, seq_len=3):\n",
    "    # Sort i split\n",
    "    df_sorted = df.sort_values(\"date\")\n",
    "    train_df, val_df, test_df = np.split(\n",
    "        df_sorted,\n",
    "        [int(0.7*len(df_sorted)), int(0.85*len(df_sorted))]\n",
    "    )\n",
    "\n",
    "    X_train_num, X_train_ids, y_train, scaler, numeric_cols = prep_df(train_df)\n",
    "    X_val_num, X_val_ids, y_val, _, _ = prep_df(val_df, scaler=scaler)\n",
    "    X_test_num, X_test_ids, y_test, _, _ = prep_df(test_df, scaler=scaler)\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = NBASeqDataset(X_train_num, X_train_ids, y_train, fetcher, seq_len)\n",
    "    val_dataset = NBASeqDataset(X_val_num, X_val_ids, y_val, fetcher, seq_len)\n",
    "    test_dataset = NBASeqDataset(X_test_num, X_test_ids, y_test, fetcher, seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    num_numeric_features = X_train_num.shape[1]\n",
    "    input_dim = seq_len * (num_numeric_features + 2*embedding_dim)\n",
    "    model = MiniNNSeq(input_dim)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Trening\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_seq, y_target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_seq)\n",
    "            loss = criterion(pred, y_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_seq.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Walidacja\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_seq, y_target in val_loader:\n",
    "                pred = model(X_seq)\n",
    "                val_loss += criterion(pred, y_target).item() * X_seq.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train MSE: {train_loss:.4f} | Val MSE: {val_loss:.4f}\")\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    home_loss = 0\n",
    "    away_loss = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_seq, y_target in test_loader:\n",
    "            pred = model(X_seq)\n",
    "            test_loss += criterion(pred, y_target).item() * X_seq.size(0)\n",
    "            home_pred, away_pred = pred[:,0], pred[:,1]\n",
    "            home_true, away_true = y_target[:,0], y_target[:,1]\n",
    "            home_loss += F.mse_loss(home_pred, home_true).item() * X_seq.size(0)\n",
    "            away_loss += F.mse_loss(away_pred, away_true).item() * X_seq.size(0)\n",
    "            total_pred = home_pred + away_pred\n",
    "            total_true = home_true + away_true\n",
    "            total_loss += F.mse_loss(total_pred, total_true).item() * X_seq.size(0)\n",
    "\n",
    "    test_size = len(test_loader.dataset)\n",
    "    test_loss /= test_size\n",
    "    home_loss /= test_size\n",
    "    away_loss /= test_size\n",
    "    total_loss /= test_size\n",
    "\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  Overall MSE (avg of home/away): {test_loss:.4f}\")\n",
    "    print(f\"  Home Points MSE: {home_loss:.4f}\")\n",
    "    print(f\"  Away Points MSE: {away_loss:.4f}\")\n",
    "    print(f\"  Total Points MSE (sum): {total_loss:.4f}\")\n",
    "\n",
    "    return model, scaler, numeric_cols, {\n",
    "        'overall_mse': test_loss,\n",
    "        'home_mse': home_loss,\n",
    "        'away_mse': away_loss,\n",
    "        'total_mse': total_loss\n",
    "    }\n",
    "\n",
    "# ------------------ RUN ------------------\n",
    "embedding_dim = home_embeddings.shape[1]\n",
    "seq_len = 3  # liczba poprzednich wektorów\n",
    "model, scaler, numeric_cols, results = train_model_seq(\n",
    "    df, embeddings_fetcher, embedding_dim, \n",
    "    num_epochs=250, lr=0.001, seq_len=seq_len\n",
    ")\n",
    "\n",
    "# ------------------ Funkcja do predykcji ------------------\n",
    "def get_predictions_seq(model, df, fetcher, scaler, numeric_cols, seq_len=3):\n",
    "    X_numeric, X_team_ids, _, _, _ = prep_df(df, scaler=scaler)\n",
    "    dataset = NBASeqDataset(X_numeric, X_team_ids, np.zeros((len(df),2)), fetcher, seq_len)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_seq, _ in DataLoader(dataset, batch_size=64, shuffle=False):\n",
    "            pred = model(X_seq)\n",
    "            predictions.append(pred.numpy())\n",
    "\n",
    "    predictions = np.vstack(predictions)\n",
    "    home_preds = predictions[:,0]\n",
    "    away_preds = predictions[:,1]\n",
    "    total_preds = home_preds + away_preds\n",
    "\n",
    "    return home_preds, away_preds, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "577a4ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['home_fga', 'away_fga', 'home_fg_pct', 'away_fg_pct', 'home_fg3a',\n",
      "       'away_fg3a', 'home_fg3_pct', 'away_fg3_pct', 'home_oreb', 'away_oreb',\n",
      "       'home_dreb', 'away_dreb', 'home_ast', 'away_ast', 'home_stl',\n",
      "       'away_stl', 'home_blk', 'away_blk', 'home_tov', 'away_tov', 'home_pf',\n",
      "       'away_pf', 'home_pts', 'away_pts', 'home_team', 'away_team', 'date',\n",
      "       'home_team_id', 'away_team_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed3b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sequences...\n",
      "Created 13956 sequences\n",
      "Sequence shape: (13956, 5, 23)\n",
      "Target shape: (13956,)\n",
      "Epoch 20/100 | Train MSE: 410.0315 | Val MSE: 158.8855\n",
      "Epoch 40/100 | Train MSE: 389.5156 | Val MSE: 153.7588\n",
      "Epoch 60/100 | Train MSE: 390.7374 | Val MSE: 160.3071\n",
      "Epoch 80/100 | Train MSE: 376.3036 | Val MSE: 158.5018\n",
      "Epoch 100/100 | Train MSE: 357.7414 | Val MSE: 156.8236\n",
      "\n",
      "Test Results:\n",
      "  Test MSE: 175.7238\n",
      "  Test MAE: 10.5372\n",
      "  Test RMSE: 13.2561\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def create_sequences(df, sequence_length=5, target_cols=['home_pts', 'away_pts']):\n",
    "    \"\"\"\n",
    "    Create sequences where we use the last N games to predict the next game.\n",
    "    This eliminates data leakage by only using historical data.\n",
    "    \"\"\"\n",
    "    # Sort by date first\n",
    "    df_sorted = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Separate teams to create sequences per team\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    team_info = []\n",
    "    \n",
    "    # Get unique teams\n",
    "    all_teams = pd.concat([df_sorted['home_team_id'], df_sorted['away_team_id']]).unique()\n",
    "    \n",
    "    for team_id in all_teams:\n",
    "        # Get all games for this team (both home and away)\n",
    "        team_games = df_sorted[\n",
    "            (df_sorted['home_team_id'] == team_id) | \n",
    "            (df_sorted['away_team_id'] == team_id)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(team_games) < sequence_length + 1:\n",
    "            continue\n",
    "            \n",
    "        # For each game, create features based on whether team was home or away\n",
    "        team_features = []\n",
    "        team_targets = []\n",
    "        \n",
    "        for _, game in team_games.iterrows():\n",
    "            if game['home_team_id'] == team_id:\n",
    "                # Team was playing at home\n",
    "                features = [\n",
    "                    game['home_fga'], game['home_fg_pct'], game['home_fg3a'], \n",
    "                    game['home_fg3_pct'], game['home_oreb'], game['home_dreb'],\n",
    "                    game['home_ast'], game['home_stl'], game['home_blk'], \n",
    "                    game['home_tov'], game['home_pf'],\n",
    "                    # Opponent features\n",
    "                    game['away_fga'], game['away_fg_pct'], game['away_fg3a'],\n",
    "                    game['away_fg3_pct'], game['away_oreb'], game['away_dreb'],\n",
    "                    game['away_ast'], game['away_stl'], game['away_blk'],\n",
    "                    game['away_tov'], game['away_pf'],\n",
    "                    1.0  # home indicator\n",
    "                ]\n",
    "                target = game['home_pts']\n",
    "            else:\n",
    "                # Team was playing away\n",
    "                features = [\n",
    "                    game['away_fga'], game['away_fg_pct'], game['away_fg3a'],\n",
    "                    game['away_fg3_pct'], game['away_oreb'], game['away_dreb'],\n",
    "                    game['away_ast'], game['away_stl'], game['away_blk'],\n",
    "                    game['away_tov'], game['away_pf'],\n",
    "                    # Opponent features  \n",
    "                    game['home_fga'], game['home_fg_pct'], game['home_fg3a'],\n",
    "                    game['home_fg3_pct'], game['home_oreb'], game['home_dreb'],\n",
    "                    game['home_ast'], game['home_stl'], game['home_blk'],\n",
    "                    game['home_tov'], game['home_pf'],\n",
    "                    0.0  # away indicator\n",
    "                ]\n",
    "                target = game['away_pts']\n",
    "            \n",
    "            team_features.append(features)\n",
    "            team_targets.append(target)\n",
    "        \n",
    "        # Create sequences for this team\n",
    "        for i in range(len(team_features) - sequence_length):\n",
    "            # Use games i to i+sequence_length-1 to predict game i+sequence_length\n",
    "            seq_features = team_features[i:i+sequence_length]\n",
    "            seq_target = team_targets[i+sequence_length]\n",
    "            \n",
    "            sequences.append(seq_features)\n",
    "            targets.append(seq_target)\n",
    "            team_info.append({\n",
    "                'team_id': team_id,\n",
    "                'game_index': i+sequence_length,\n",
    "                'date': team_games.iloc[i+sequence_length]['date']\n",
    "            })\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), team_info\n",
    "\n",
    "class SequentialNBADataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "class TrueSequentialNBAModel(nn.Module):\n",
    "    def __init__(self, input_features, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(TrueSequentialNBAModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM to process the sequence of games\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_features)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use the last output from the sequence\n",
    "        last_output = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Predict the score\n",
    "        output = self.fc(last_output)\n",
    "        return output.squeeze(-1)  # Shape: (batch_size,)\n",
    "\n",
    "def train_sequential_model(df, sequence_length=5, num_epochs=100, lr=0.001, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the sequential model without data leakage\n",
    "    \"\"\"\n",
    "    # Create sequences\n",
    "    print(\"Creating sequences...\")\n",
    "    sequences, targets, team_info = create_sequences(df, sequence_length)\n",
    "    \n",
    "    print(f\"Created {len(sequences)} sequences\")\n",
    "    print(f\"Sequence shape: {sequences.shape}\")\n",
    "    print(f\"Target shape: {targets.shape}\")\n",
    "    \n",
    "    # Split data temporally (important for time series)\n",
    "    # Use first 70% for training, next 15% for validation, last 15% for testing\n",
    "    n_samples = len(sequences)\n",
    "    train_idx = int(0.7 * n_samples)\n",
    "    val_idx = int(0.85 * n_samples)\n",
    "    \n",
    "    train_sequences = sequences[:train_idx]\n",
    "    train_targets = targets[:train_idx]\n",
    "    \n",
    "    val_sequences = sequences[train_idx:val_idx]\n",
    "    val_targets = targets[train_idx:val_idx]\n",
    "    \n",
    "    test_sequences = sequences[val_idx:]\n",
    "    test_targets = targets[val_idx:]\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = SequentialNBADataset(train_sequences, train_targets)\n",
    "    val_dataset = SequentialNBADataset(val_sequences, val_targets)\n",
    "    test_dataset = SequentialNBADataset(test_sequences, test_targets)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_features = sequences.shape[2]  # Number of features per game\n",
    "    model = TrueSequentialNBAModel(input_features)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences_batch, targets_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences_batch)\n",
    "            loss = criterion(predictions, targets_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * sequences_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences_batch, targets_batch in val_loader:\n",
    "                predictions = model(sequences_batch)\n",
    "                val_loss += criterion(predictions, targets_batch).item() * sequences_batch.size(0)\n",
    "        \n",
    "        val_loss /= len(val_dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train MSE: {train_loss:.4f} | Val MSE: {val_loss:.4f}\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences_batch, targets_batch in test_loader:\n",
    "            predictions = model(sequences_batch)\n",
    "            test_loss += criterion(predictions, targets_batch).item() * sequences_batch.size(0)\n",
    "            all_predictions.append(predictions.numpy())\n",
    "            all_targets.append(targets_batch.numpy())\n",
    "    \n",
    "    test_loss /= len(test_dataset)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Test MSE: {test_loss:.4f}\")\n",
    "    print(f\"  Test MAE: {mae:.4f}\")\n",
    "    print(f\"  Test RMSE: {np.sqrt(test_loss):.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses, test_loss\n",
    "\n",
    "# Usage example:\n",
    "model, train_losses, val_losses, test_loss = train_sequential_model(df, sequence_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "145ebab0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_numeric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# ---- Użycie ----\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Zakładam, że masz przygotowane:\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# X_numeric, X_team_ids, y, fetcher\u001b[39;00m\n\u001b[32m     88\u001b[39m seq_len = \u001b[32m3\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m model = train_seq_model(\u001b[43mX_numeric\u001b[49m, X_team_ids, y, embeddings_fetcher, seq_len=seq_len)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_numeric' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# ---- Przygotowanie sekwencyjnego datasetu ----\n",
    "class NBASeqDataset(Dataset):\n",
    "    def __init__(self, X_numeric, X_team_ids, y, fetcher, seq_len=3):\n",
    "        \"\"\"\n",
    "        X_numeric: (num_samples, num_features)\n",
    "        X_team_ids: (num_samples, 2) -> home_id, away_id\n",
    "        y: (num_samples, 2) -> home_pts, away_pts\n",
    "        fetcher: EmbeddingFetcher\n",
    "        seq_len: ile poprzednich wektorów użyć\n",
    "        \"\"\"\n",
    "        self.X_numeric = torch.tensor(X_numeric, dtype=torch.float32)\n",
    "        self.X_team_ids = torch.tensor(X_team_ids, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.fetcher = fetcher\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y) - self.seq_len  # bo używamy poprzednich seq_len wektorów\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # pobierz sekwencję\n",
    "        seq_numeric = self.X_numeric[idx:idx+self.seq_len].flatten()  # spłaszczamy wszystkie wektory\n",
    "        seq_home_emb = torch.cat([\n",
    "            torch.tensor(self.fetcher.get_home_embedding(int(self.X_team_ids[i,0])), dtype=torch.float32)\n",
    "            for i in range(idx, idx+self.seq_len)\n",
    "        ])\n",
    "        seq_away_emb = torch.cat([\n",
    "            torch.tensor(self.fetcher.get_away_embedding(int(self.X_team_ids[i,1])), dtype=torch.float32)\n",
    "            for i in range(idx, idx+self.seq_len)\n",
    "        ])\n",
    "        X_seq = torch.cat([seq_numeric, seq_home_emb, seq_away_emb])\n",
    "        y_target = self.y[idx+self.seq_len]  # przewidujemy wektor w momencie t\n",
    "        return X_seq, y_target\n",
    "\n",
    "# ---- Prosty NN dla sekwencji ----\n",
    "class MiniNNSeq(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MiniNNSeq, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)  # przewidujemy home i away points\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ---- Funkcja do trenowania ----\n",
    "def train_seq_model(X_numeric, X_team_ids, y, fetcher, seq_len=3, num_epochs=100, lr=0.001, batch_size=64):\n",
    "    dataset = NBASeqDataset(X_numeric, X_team_ids, y, fetcher, seq_len)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # input_dim = seq_len * (num_numeric_features + 2*embedding_dim)\n",
    "    num_numeric = X_numeric.shape[1]\n",
    "    embedding_dim = fetcher.home_embeddings.shape[1]\n",
    "    input_dim = seq_len * (num_numeric + 2*embedding_dim)\n",
    "    \n",
    "    model = MiniNNSeq(input_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_seq, y_target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_seq)\n",
    "            loss = criterion(pred, y_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_seq.size(0)\n",
    "        epoch_loss /= len(dataset)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train MSE: {epoch_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# ---- Użycie ----\n",
    "# Zakładam, że masz przygotowane:\n",
    "# X_numeric, X_team_ids, y, fetcher\n",
    "seq_len = 3\n",
    "model = train_seq_model(X_numeric, X_team_ids, y, embeddings_fetcher, seq_len=seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d863fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pre-game features...\n",
      "Created 7005 samples with 17 pre-game features each\n",
      "Starting training...\n",
      "Epoch 25/250 | Train MSE: 224.6755 | Val MSE: 194.2977\n",
      "Epoch 50/250 | Train MSE: 208.5151 | Val MSE: 200.5405\n",
      "Epoch 75/250 | Train MSE: 199.0594 | Val MSE: 226.9470\n",
      "Epoch 100/250 | Train MSE: 193.4310 | Val MSE: 226.8276\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 378\u001b[39m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, scaler, feature_names, {\n\u001b[32m    370\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moverall_mse\u001b[39m\u001b[33m'\u001b[39m: test_loss,\n\u001b[32m    371\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhome_mse\u001b[39m\u001b[33m'\u001b[39m: home_loss,\n\u001b[32m   (...)\u001b[39m\u001b[32m    374\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m'\u001b[39m: mae\n\u001b[32m    375\u001b[39m     }\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m model, scaler, feature_names, results = \u001b[43mtrain_model_seq_fixed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m     \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m     \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m    381\u001b[39m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 301\u001b[39m, in \u001b[36mtrain_model_seq_fixed\u001b[39m\u001b[34m(df, fetcher, embedding_dim, num_epochs, lr, batch_size, seq_len)\u001b[39m\n\u001b[32m    299\u001b[39m model.train()\n\u001b[32m    300\u001b[39m train_loss = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_seq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\szymo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\szymo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\szymo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 212\u001b[39m, in \u001b[36mNBASeqDatasetFixed.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    208\u001b[39m seq_numeric = \u001b[38;5;28mself\u001b[39m.X_numeric[idx:idx+\u001b[38;5;28mself\u001b[39m.seq_len].flatten()\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Get team embeddings for sequence\u001b[39;00m\n\u001b[32m    211\u001b[39m seq_home_emb = torch.cat([\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_home_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_team_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(idx, idx+\u001b[38;5;28mself\u001b[39m.seq_len)\n\u001b[32m    214\u001b[39m ])\n\u001b[32m    215\u001b[39m seq_away_emb = torch.cat([\n\u001b[32m    216\u001b[39m     torch.tensor(\u001b[38;5;28mself\u001b[39m.fetcher.get_away_embedding(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.X_team_ids[i,\u001b[32m1\u001b[39m])), dtype=torch.float32)\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(idx, idx+\u001b[38;5;28mself\u001b[39m.seq_len)\n\u001b[32m    218\u001b[39m ])\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Combine all features\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def create_pregame_features_per_game(df):\n",
    "    \"\"\"\n",
    "    Create pre-game features for each game that would be available before tipoff\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    features_list = []\n",
    "    targets_list = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i in range(len(df_sorted)):\n",
    "        current_game = df_sorted.iloc[i]\n",
    "        home_team = current_game['home_team_id']\n",
    "        away_team = current_game['away_team_id']\n",
    "        \n",
    "        # Get historical data (games before this one)\n",
    "        historical = df_sorted.iloc[:i]\n",
    "        \n",
    "        if len(historical) < 10:  # Need minimum historical data\n",
    "            continue\n",
    "        \n",
    "        # Calculate pre-game features\n",
    "        home_history = historical[\n",
    "            (historical['home_team_id'] == home_team) | \n",
    "            (historical['away_team_id'] == home_team)\n",
    "        ]\n",
    "        away_history = historical[\n",
    "            (historical['home_team_id'] == away_team) | \n",
    "            (historical['away_team_id'] == away_team)\n",
    "        ]\n",
    "        \n",
    "        if len(home_history) < 3 or len(away_history) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Home team stats (from completed games)\n",
    "        home_total_pts = 0\n",
    "        home_total_allowed = 0\n",
    "        home_wins = 0\n",
    "        home_games = len(home_history)\n",
    "        home_home_wins = 0\n",
    "        home_home_games = 0\n",
    "        \n",
    "        for _, game in home_history.iterrows():\n",
    "            if game['home_team_id'] == home_team:  # Was home\n",
    "                home_total_pts += game['home_pts']\n",
    "                home_total_allowed += game['away_pts']\n",
    "                home_home_games += 1\n",
    "                if game['home_pts'] > game['away_pts']:\n",
    "                    home_wins += 1\n",
    "                    home_home_wins += 1\n",
    "            else:  # Was away\n",
    "                home_total_pts += game['away_pts']\n",
    "                home_total_allowed += game['home_pts']\n",
    "                if game['away_pts'] > game['home_pts']:\n",
    "                    home_wins += 1\n",
    "        \n",
    "        # Away team stats\n",
    "        away_total_pts = 0\n",
    "        away_total_allowed = 0\n",
    "        away_wins = 0\n",
    "        away_games = len(away_history)\n",
    "        away_away_wins = 0\n",
    "        away_away_games = 0\n",
    "        \n",
    "        for _, game in away_history.iterrows():\n",
    "            if game['home_team_id'] == away_team:  # Was home\n",
    "                away_total_pts += game['home_pts']\n",
    "                away_total_allowed += game['away_pts']\n",
    "                if game['home_pts'] > game['away_pts']:\n",
    "                    away_wins += 1\n",
    "            else:  # Was away\n",
    "                away_total_pts += game['away_pts']\n",
    "                away_total_allowed += game['home_pts']\n",
    "                away_away_games += 1\n",
    "                if game['away_pts'] > game['home_pts']:\n",
    "                    away_wins += 1\n",
    "                    away_away_wins += 1\n",
    "        \n",
    "        # Head to head\n",
    "        h2h = historical[\n",
    "            ((historical['home_team_id'] == home_team) & (historical['away_team_id'] == away_team)) |\n",
    "            ((historical['home_team_id'] == away_team) & (historical['away_team_id'] == home_team))\n",
    "        ]\n",
    "        \n",
    "        h2h_games = len(h2h)\n",
    "        h2h_home_wins = 0\n",
    "        if h2h_games > 0:\n",
    "            for _, game in h2h.iterrows():\n",
    "                if ((game['home_team_id'] == home_team) and (game['home_pts'] > game['away_pts'])) or \\\n",
    "                   ((game['away_team_id'] == home_team) and (game['away_pts'] > game['home_pts'])):\n",
    "                    h2h_home_wins += 1\n",
    "        \n",
    "        # Recent form (last 3 games)\n",
    "        home_recent = home_history.tail(3)\n",
    "        away_recent = away_history.tail(3)\n",
    "        \n",
    "        home_recent_wins = 0\n",
    "        home_recent_pts = 0\n",
    "        for _, game in home_recent.iterrows():\n",
    "            if game['home_team_id'] == home_team:\n",
    "                home_recent_pts += game['home_pts']\n",
    "                if game['home_pts'] > game['away_pts']:\n",
    "                    home_recent_wins += 1\n",
    "            else:\n",
    "                home_recent_pts += game['away_pts']\n",
    "                if game['away_pts'] > game['home_pts']:\n",
    "                    home_recent_wins += 1\n",
    "        \n",
    "        away_recent_wins = 0\n",
    "        away_recent_pts = 0\n",
    "        for _, game in away_recent.iterrows():\n",
    "            if game['home_team_id'] == away_team:\n",
    "                away_recent_pts += game['home_pts']\n",
    "                if game['home_pts'] > game['away_pts']:\n",
    "                    away_recent_wins += 1\n",
    "            else:\n",
    "                away_recent_pts += game['away_pts']\n",
    "                if game['away_pts'] > game['home_pts']:\n",
    "                    away_recent_wins += 1\n",
    "        \n",
    "        # Compile features\n",
    "        game_features = [\n",
    "            # Home team strength\n",
    "            home_wins / home_games,  # Overall win rate\n",
    "            home_total_pts / home_games,  # PPG\n",
    "            home_total_allowed / home_games,  # Opponent PPG\n",
    "            home_home_wins / home_home_games if home_home_games > 0 else 0.5,  # Home win rate\n",
    "            home_games,  # Experience\n",
    "            \n",
    "            # Away team strength\n",
    "            away_wins / away_games,  # Overall win rate\n",
    "            away_total_pts / away_games,  # PPG\n",
    "            away_total_allowed / away_games,  # Opponent PPG\n",
    "            away_away_wins / away_away_games if away_away_games > 0 else 0.5,  # Away win rate\n",
    "            away_games,  # Experience\n",
    "            \n",
    "            # Head to head\n",
    "            h2h_games,\n",
    "            h2h_home_wins / h2h_games if h2h_games > 0 else 0.5,\n",
    "            \n",
    "            # Recent form\n",
    "            home_recent_wins / len(home_recent) if len(home_recent) > 0 else 0.5,\n",
    "            home_recent_pts / len(home_recent) if len(home_recent) > 0 else 110,\n",
    "            away_recent_wins / len(away_recent) if len(away_recent) > 0 else 0.5,\n",
    "            away_recent_pts / len(away_recent) if len(away_recent) > 0 else 110,\n",
    "            \n",
    "            # Context\n",
    "            1.0,  # Home court advantage\n",
    "        ]\n",
    "        \n",
    "        features_list.append(game_features)\n",
    "        targets_list.append([current_game['home_pts'], current_game['away_pts']])\n",
    "        valid_indices.append(i)\n",
    "    \n",
    "    return np.array(features_list), np.array(targets_list), valid_indices\n",
    "\n",
    "def prep_pregame_df(df, target_cols=['home_pts', 'away_pts'], scaler=None):\n",
    "    \"\"\"\n",
    "    Prepare dataframe with pre-game features instead of box score stats\n",
    "    \"\"\"\n",
    "    X_features, y, valid_indices = create_pregame_features_per_game(df)\n",
    "    \n",
    "    # Get team IDs for the valid games\n",
    "    df_valid = df.iloc[valid_indices]\n",
    "    X_team_ids = df_valid[['home_team_id', 'away_team_id']].astype(int).values\n",
    "    \n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_numeric = scaler.fit_transform(X_features)\n",
    "    else:\n",
    "        X_numeric = scaler.transform(X_features)\n",
    "    \n",
    "    feature_names = [\n",
    "        'home_win_rate', 'home_ppg', 'home_opp_ppg', 'home_home_win_rate', 'home_games',\n",
    "        'away_win_rate', 'away_ppg', 'away_opp_ppg', 'away_away_win_rate', 'away_games',\n",
    "        'h2h_games', 'h2h_home_win_rate',\n",
    "        'home_recent_win_rate', 'home_recent_ppg', 'away_recent_win_rate', 'away_recent_ppg',\n",
    "        'home_court_advantage'\n",
    "    ]\n",
    "    \n",
    "    return X_numeric, X_team_ids, y, scaler, feature_names\n",
    "\n",
    "# Modified dataset class\n",
    "class NBASeqDatasetFixed(Dataset):\n",
    "    def __init__(self, X_numeric, X_team_ids, y, fetcher, seq_len=3):\n",
    "        self.X_numeric = torch.tensor(X_numeric, dtype=torch.float32)\n",
    "        self.X_team_ids = torch.tensor(X_team_ids, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.fetcher = fetcher\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.y) - self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create sequence of pre-game features\n",
    "        seq_numeric = self.X_numeric[idx:idx+self.seq_len].flatten()\n",
    "        \n",
    "        # Get team embeddings for sequence\n",
    "        seq_home_emb = torch.cat([\n",
    "            torch.tensor(self.fetcher.get_home_embedding(int(self.X_team_ids[i,0])), dtype=torch.float32)\n",
    "            for i in range(idx, idx+self.seq_len)\n",
    "        ])\n",
    "        seq_away_emb = torch.cat([\n",
    "            torch.tensor(self.fetcher.get_away_embedding(int(self.X_team_ids[i,1])), dtype=torch.float32)\n",
    "            for i in range(idx, idx+self.seq_len)\n",
    "        ])\n",
    "        \n",
    "        # Combine all features\n",
    "        X_seq = torch.cat([seq_numeric, seq_home_emb, seq_away_emb])\n",
    "        y_target = self.y[idx+self.seq_len]  # Predict the next game\n",
    "        \n",
    "        return X_seq, y_target\n",
    "\n",
    "# Same model class\n",
    "class MiniNNSeq(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MiniNNSeq, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Modified training function\n",
    "def train_model_seq_fixed(df, fetcher, embedding_dim, num_epochs=250, lr=0.001, batch_size=64, seq_len=3):\n",
    "    print(\"Creating pre-game features...\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values(\"date\")\n",
    "    \n",
    "    # Create pre-game features for all data first\n",
    "    X_all, X_team_ids_all, y_all, scaler, feature_names = prep_pregame_df(df_sorted)\n",
    "    \n",
    "    print(f\"Created {len(X_all)} samples with {len(feature_names)} pre-game features each\")\n",
    "    \n",
    "    if len(X_all) < 100:\n",
    "        print(\"Not enough samples for training!\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Split data temporally\n",
    "    train_idx = int(0.7 * len(X_all))\n",
    "    val_idx = int(0.85 * len(X_all))\n",
    "    \n",
    "    X_train_num = X_all[:train_idx]\n",
    "    X_train_ids = X_team_ids_all[:train_idx]\n",
    "    y_train = y_all[:train_idx]\n",
    "    \n",
    "    X_val_num = X_all[train_idx:val_idx]\n",
    "    X_val_ids = X_team_ids_all[train_idx:val_idx]\n",
    "    y_val = y_all[train_idx:val_idx]\n",
    "    \n",
    "    X_test_num = X_all[val_idx:]\n",
    "    X_test_ids = X_team_ids_all[val_idx:]\n",
    "    y_test = y_all[val_idx:]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NBASeqDatasetFixed(X_train_num, X_train_ids, y_train, fetcher, seq_len)\n",
    "    val_dataset = NBASeqDatasetFixed(X_val_num, X_val_ids, y_val, fetcher, seq_len)\n",
    "    test_dataset = NBASeqDatasetFixed(X_test_num, X_test_ids, y_test, fetcher, seq_len)\n",
    "\n",
    "    if len(train_dataset) < 10:\n",
    "        print(\"Not enough sequential samples for training!\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    num_features = X_train_num.shape[1]\n",
    "    input_dim = seq_len * (num_features + 2*embedding_dim)\n",
    "    model = MiniNNSeq(input_dim)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_seq, y_target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_seq)\n",
    "            loss = criterion(pred, y_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_seq.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_seq, y_target in val_loader:\n",
    "                pred = model(X_seq)\n",
    "                val_loss += criterion(pred, y_target).item() * X_seq.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        if (epoch+1) % 25 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train MSE: {train_loss:.4f} | Val MSE: {val_loss:.4f}\")\n",
    "\n",
    "    # Test evaluation\n",
    "    print(\"Evaluating on test set...\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    home_loss = 0\n",
    "    away_loss = 0\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_seq, y_target in test_loader:\n",
    "            pred = model(X_seq)\n",
    "            test_loss += criterion(pred, y_target).item() * X_seq.size(0)\n",
    "            \n",
    "            home_pred, away_pred = pred[:,0], pred[:,1]\n",
    "            home_true, away_true = y_target[:,0], y_target[:,1]\n",
    "            \n",
    "            home_loss += F.mse_loss(home_pred, home_true).item() * X_seq.size(0)\n",
    "            away_loss += F.mse_loss(away_pred, away_true).item() * X_seq.size(0)\n",
    "            \n",
    "            total_pred = home_pred + away_pred\n",
    "            total_true = home_true + away_true\n",
    "            total_loss += F.mse_loss(total_pred, total_true).item() * X_seq.size(0)\n",
    "            \n",
    "            all_preds.append(pred.numpy())\n",
    "            all_targets.append(y_target.numpy())\n",
    "\n",
    "    test_size = len(test_loader.dataset)\n",
    "    test_loss /= test_size\n",
    "    home_loss /= test_size\n",
    "    away_loss /= test_size\n",
    "    total_loss /= test_size\n",
    "    \n",
    "    # Calculate MAE\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    mae = np.mean(np.abs(all_preds - all_targets))\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Overall MSE: {test_loss:.4f}\")\n",
    "    print(f\"  Home Points MSE: {home_loss:.4f}\")\n",
    "    print(f\"  Away Points MSE: {away_loss:.4f}\")\n",
    "    print(f\"  Total Points MSE: {total_loss:.4f}\")\n",
    "    print(f\"  Overall MAE: {mae:.4f}\")\n",
    "    print(f\"  RMSE: {np.sqrt(test_loss):.4f}\")\n",
    "\n",
    "    return model, scaler, feature_names, {\n",
    "        'overall_mse': test_loss,\n",
    "        'home_mse': home_loss,\n",
    "        'away_mse': away_loss,\n",
    "        'total_mse': total_loss,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "# Usage:\n",
    "model, scaler, feature_names, results = train_model_seq_fixed(\n",
    "     df, embeddings_fetcher, embedding_dim, \n",
    "     num_epochs=250, lr=0.001, seq_len=3\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
